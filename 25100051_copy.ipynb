{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shaheer2812/JavaScript30/blob/master/25100051_copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9UtJfIIGbCq"
      },
      "source": [
        "# PA1: K-Nearest Neighbors\n",
        "\n",
        "<center>\n",
        "    <img src=\"./assets/nn-k1.png\">\n",
        "</center>\n",
        "\n",
        "### Introduction\n",
        "\n",
        "In this assignment, you will be creating your first Machine Learning model from scratch: K-Nearest Neighbors.\n",
        "\n",
        "This algorithm is one of the simpler ones you will come across, but the ideas can be applied to large-scale sophisticated systems: Semantic Search and Recommendation Systems for starters.\n",
        "\n",
        "For this assignment, you will be creating your own KNN-classifier from scratch using `numpy`. You can then use this to classify images of _handwritten digits_ from the [MNIST dataset](http://yann.lecun.com/exdb/mnist/). This is the \"Hello World\" of Machine Learning.\n",
        "\n",
        "After this notebook you should be able to:\n",
        "\n",
        "- Utilize `numpy` to implement a simple KNN classifier from scratch\n",
        "\n",
        "- Understand how to setup a good Cross Validation strategy\n",
        "\n",
        "- Be able to setup simple classification tasks\n",
        "\n",
        "### Instructions\n",
        "\n",
        "- Follow along with the notebook, filling out the necessary code where instructed.\n",
        "\n",
        "- <span style=\"color: red;\">Read the Submission Instructions and Plagiarism Policy in the attached PDF.</span>\n",
        "\n",
        "- <span style=\"color: red;\">Make sure to run all cells for credit.</span>\n",
        "\n",
        "- <span style=\"color: red;\">Do not remove any pre-written code.</span> We will be using the `print` statements to grade your assignment.\n",
        "\n",
        "- <span style=\"color: red;\">You must attempt all parts.</span> Do not assume that because something is for 0 marks, you can leave it - it will definitely be used in later parts.\n",
        "\n",
        "- <span style=\"color: red;\">Do not use unauthorized libraries.</span> You are not allowed to use `sklearn` in Part 1. Failure to follow these instructions will result in a serious penalty.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMxzaKw93VoI"
      },
      "source": [
        "## Part 1: KNNs from Scratch [75 marks]\n",
        "\n",
        "Again, you are <span style=\"color: red;\">not allowed</span> to use scikit-learn or any other machine learning toolkit for this part. You have to implement your own k-NN classifier from scratch.\n",
        "\n",
        "You can use `numpy`, `pandas`, `seaborn`, `matplotlib`, `PIL`, and the standard Python libraries for this part. Contact the TAs if you want to use any other libraries.\n",
        "\n",
        "### Importing Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "TjOjOx_YwyII",
        "outputId": "b4400331-7cc0-459a-c75a-b1b11631f036"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5184b94deb3e>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m     case = d.expect([\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pexpect/spawnbase.py\u001b[0m in \u001b[0;36mexpect\u001b[0;34m(self, pattern, timeout, searchwindowsize, async_, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mcompiled_pattern_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_pattern_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         return self.expect_list(compiled_pattern_list,\n\u001b[0m\u001b[1;32m    344\u001b[0m                 timeout, searchwindowsize, async_)\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pexpect/spawnbase.py\u001b[0m in \u001b[0;36mexpect_list\u001b[0;34m(self, pattern_list, timeout, searchwindowsize, async_, **kw)\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mexpect_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpect_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     def expect_exact(self, pattern_list, timeout=-1, searchwindowsize=-1,\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pexpect/expect.py\u001b[0m in \u001b[0;36mexpect_loop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mincoming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_nonblocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayafterread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayafterread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m                 \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincoming\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# Keep reading until exception or return.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import PIL\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxZ0oTZ1Lwf4"
      },
      "source": [
        "### Loading the dataset\n",
        "\n",
        "<center>\n",
        "    <img src=\"./assets/mnist.png\">\n",
        "</center>\n",
        "\n",
        "The MNIST dataset consists of 70,000 labeled images of handwritten digits, each of size 28 pixels by 28 pixels.\n",
        "\n",
        "The dataset given to you is in a CSV file, `mnist.csv`. The CSV file has ~70,000 rows and 785 columns. You can download it using [this link](https://drive.google.com/file/d/16STvH3jEk-JF1BGAguhnA9y0sT_c_MC-/view?usp=sharing).\n",
        "\n",
        "- Each row represents one image of a handwritten digit. Note that the header row contains the column names.\n",
        "\n",
        "- The first column gives the label (a number from 0 to 9). The next 784 columns give the value of each pixel. There are 784 pixels in each image corresponding to an image size of 28 by 28.\n",
        "\n",
        "For faster prototyping, you can sample 20% of the entire dataset.\n",
        "\n",
        "You can use the `pandas` library to load the CSV file but the final dataset should be stored in a `numpy` array of shape (14000, 785).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUjljzTQ1rN_"
      },
      "outputs": [],
      "source": [
        "## (3 marks)\n",
        "\n",
        "file_path = '/content/drive/My Drive/mnist.csv'\n",
        "df = pd.read_csv(file_path, header=1)\n",
        "# df.drop(df.index[0])\n",
        "arr = df.to_numpy()\n",
        "# arr = arr.astype(int)\n",
        "\n",
        "# print(f\"shape : {arr.shape}\")\n",
        "\n",
        "\n",
        "# TODO: Print the shapes\n",
        "print(f\"Shape : {arr.shape}\")\n",
        "\n",
        "\n",
        "# TODO: Display the first 5 rows of the dataset\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FiUkM6yw6SX"
      },
      "outputs": [],
      "source": [
        "# (2 marks)\n",
        "# TODO: Sample 20% of the dataset (Make sure this is a random sample!)\n",
        "# Please note that this is not the train-test split. This is just a sample of the dataset. We are doing this to reduce the computation time.\n",
        "sampled = df.sample(n=14000, random_state=22)\n",
        "arr = np.array(sampled)\n",
        "# new = np.delete(arr, 0, axis=1)\n",
        "print(arr[0][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d2EtnB9PB8x"
      },
      "source": [
        "### Displaying Images\n",
        "\n",
        "Now that you've loaded the dataset, let's display some images.\n",
        "\n",
        "You can reshape these 784 values for each image, into a `28x28` array, then use either `matplotlib` or `PIL` to display the image.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZ7AbgWjPA4h"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement a display_image function (3 marks)\n",
        "\n",
        "iterations = 1\n",
        "\n",
        "\n",
        "def display_image(arr):\n",
        "    \"\"\"\n",
        "    Takes a 1D numpy array, reshapes to a 28x28 array and displays the image\n",
        "    \"\"\"\n",
        "    # print(arr)\n",
        "    # print(len(arr))\n",
        "    new = np.delete(arr, 0)\n",
        "    # print(len(new))\n",
        "    new = new.reshape(28, 28)\n",
        "    # print(new.shape)\n",
        "    # print(new)\n",
        "    # print(f\"this number is : {arr[0]}\")\n",
        "    plt.subplot(1, 5, iterations)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvuajHXBqCIT"
      },
      "outputs": [],
      "source": [
        "# TODO: Randomly pick and display 5 images from the dataset\n",
        "\n",
        "## Code here\n",
        "\n",
        "# pick a random row\n",
        "for _ in range(5):\n",
        "    random_index = np.random.randint(0, len(arr))\n",
        "    random_img = arr[random_index, :]\n",
        "    print(arr[random_index, 0])\n",
        "    display_image(random_img)\n",
        "    iterations += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ND2Dc_EOQzCg"
      },
      "source": [
        "### Train-Test split\n",
        "\n",
        "With the data loaded, you should set up a proper Cross Validation scheme for your modeling experiments, before you actually start building your model.\n",
        "\n",
        "Divide the dataset into training and test sets (around an 85-15 split). More precisely, take the first 11900 images for the training set and the last 2100 for the test set.\n",
        "\n",
        "Both the resulting splits/sets should be stored in `numpy` arrays of shape `(num_split_images, 785)`. Depending on your approach, you can also separate the labels into a different array (or two arrays)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iBwQIMsQ1wd"
      },
      "outputs": [],
      "source": [
        "## (3 marks)\n",
        "# TODO: Create a train-test split (2 marks)\n",
        "labels = arr[:, 0]\n",
        "# print((labels.shape))\n",
        "\n",
        "data_train = arr[:11900, :]\n",
        "data_test = arr[11900:, :]\n",
        "print(data_train[0][0])\n",
        "\n",
        "\n",
        "# TODO:Print the shapes of both arrays (1 mark)\n",
        "print(f\"Shape data_train : {data_train.shape}\\nShape data_test : {data_test.shape}\")\n",
        "# print(labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ro0MSTLjFiuL"
      },
      "outputs": [],
      "source": [
        "## (3 marks)\n",
        "# TODO: Split the train and test into inputs and labels (2 marks)\n",
        "input_train = data_train[:, 1:]\n",
        "input_test = data_test[:, 1:]\n",
        "\n",
        "label_train = labels[:11900]\n",
        "label_test = labels[11900:]\n",
        "\n",
        "# TODO: Print the shapes of the 4 arrays (1 mark)\n",
        "print(\n",
        "    f\"input_train: {input_train.shape}\\ninput_test: {input_test.shape}\\nlabel_train: {label_train.shape}\\nlabel_test: {label_test.shape}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUX1FaJfpVb0"
      },
      "source": [
        "### Implementing k-NN Classifier\n",
        "\n",
        "#### Instructions\n",
        "\n",
        "Now you can create your own k-NN classifier. You can use the following steps as a guide:\n",
        "\n",
        "1. For a test data point, find its distance from all training instances.\n",
        "\n",
        "2. Sort the calculated distances in ascending order based on distance values.\n",
        "\n",
        "3. Choose k training samples with minimum distances from the test data point.\n",
        "\n",
        "4. Return the _most frequent_ class of these samples.\n",
        "\n",
        "**Note:** Your function should work with _Euclidean_ distance as well as _Manhattan_ distance. Pass the distance metric as a parameter in the k-NN classifier function. Your function should also let one specify the value of `k`.\n",
        "\n",
        "For values of `k` where a tie occurs, you need to break the tie by backing off to the `k-1` value. In case there is still a tie, you will continue decreasing `k` until there is a clear winner.\n",
        "\n",
        "#### Distance functions\n",
        "\n",
        "First, implement separate functions for the Euclidean and Manhattan distances. Formulas for both are given below.\n",
        "\n",
        "$$\n",
        "d_{\\text{Euclidean}}(\\vec{p},\\vec{q}) = \\sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2 + (p_3 - q_3)^2 + ... + (p_n - q_n)^2}\n",
        "$$\n",
        "\n",
        "$$\n",
        "d_{\\text{Manhattan}}(\\vec{p},\\vec{q}) = |(p_1 - q_1)| + |(p_2 - q_2)| + |(p_3 - q_3)| + ... + |(p_n - q_n)|\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_d287bYwx-B_"
      },
      "outputs": [],
      "source": [
        "def euclidean_distance(vector1, vector2):\n",
        "    # TODO:Implement the euclidean distance function (2 marks)\n",
        "\n",
        "    t = vector1 - vector2\n",
        "    ans = np.sqrt(np.dot(t.T, t))\n",
        "    return ans\n",
        "\n",
        "\n",
        "def manhattan_distance(vector1, vector2):\n",
        "    # TODO:Implement Euclidean and Manhattan distance functions (2 marks)\n",
        "    ans = np.sum(np.abs(vector1 - vector2))\n",
        "    return ans\n",
        "\n",
        "\n",
        "#########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLLQPLGZYLus"
      },
      "source": [
        "#### k-NN Classifier methods\n",
        "\n",
        "Complete the following method functions:\n",
        "\n",
        "1. `fit`\n",
        "\n",
        "2. `get_neighbours`\n",
        "\n",
        "3. `predict`\n",
        "\n",
        "You can make as many helper functions as you need.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-vNkZ1dOtXz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class kNearestNeighbours:\n",
        "\tdef __init__(self, num_neibrs):\n",
        "\t\t### DO NOT EDIT !! ###\n",
        "\t\t\"\"\"\n",
        "\t\tn_neighbours: value of k\n",
        "\t\tX: array of training data points\n",
        "\t\ty: array of gold labels for training points\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tself.n_neighbors = num_neibrs\n",
        "\t\tself.X = None\n",
        "\t\tself.y = None\n",
        "\n",
        "\t#######################\n",
        "\t#data and its labels passed\n",
        "\tdef fit(self, X_train, y_train):\n",
        "\t\t\"\"\"\n",
        "\t\tFit the training data to the model\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\t# TODO: Set both attributes (1 mark)\n",
        "\n",
        "\t\t#x_train has 784 col, y_train are the labels\n",
        "\t\tself.X = X_train\n",
        "\t\tself.y = y_train\n",
        "\n",
        "\n",
        "\tdef get_neighbors(self, x, distanceFunction):\n",
        "\t\t\"\"\"\n",
        "\t\tReturn the k nearest neighbours of the input data point x.\n",
        "\t\tHint: you can even just return the indices of the data points\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\t# TODO: Complete the get_neighbors function (4 marks)\n",
        "\t\tans = []\n",
        "\t\tfor i in range(0, len(self.X)):\n",
        "\t\t\ttemp = self.X[i, :]\n",
        "\n",
        "\t\t\t# #if the same point used\n",
        "\t\t\t# if np.array_equal(x, temp):\n",
        "\t\t\t# \tcontinue\n",
        "\n",
        "\t\t\td = distanceFunction(x, temp)\n",
        "\t\t\tans.append((d, i))\n",
        "\n",
        "\t\tans.sort()\n",
        "\n",
        "\t\t#choosing the k nearest points\n",
        "\t\tslice_ans = ans[0:self.n_neighbors]\n",
        "\t\tindices = []\n",
        "\t\tfor i in range(0, self.n_neighbors):\n",
        "\t\t\ttemp = slice_ans[i]\n",
        "\t\t\ttemp = temp[1]\n",
        "\t\t\t#the k nearest indices\n",
        "\t\t\tindices.append(temp)\n",
        "\n",
        "\t\treturn indices\n",
        "\n",
        "\tdef predict(self, X_test, distanceFunction):\n",
        "\t\t\"\"\"\n",
        "\t\tReturns an array of predicted labels for all points in the X_test array\n",
        "\n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tX_test : array\n",
        "\t\t\t\t\t\tThe test data\n",
        "\n",
        "\t\tdistanceFunction : function\n",
        "\t\t\t\t\t\tThe distance function to be used\n",
        "\t\t\"\"\"\n",
        "\t\t# TODO: Complete the predict function (5 marks)\n",
        "\t\tans_arr = []\n",
        "\n",
        "\t\tfor i in range(0, len(X_test)):\n",
        "\t\t\t#getting k narest neighbours for every point in the test data\n",
        "\t\t\tindices = self.get_neighbors(X_test[i], distanceFunction)\n",
        "\t\t\tflag = True\n",
        "\n",
        "\t\t\tk = self.n_neighbors\n",
        "\t\t\t#run the loop as long as there is no tie\n",
        "\t\t\twhile flag:\n",
        "\n",
        "\t\t\t\t#stores the closest n neighbours\n",
        "\t\t\t\tmax_count = [-1, -1]\n",
        "\n",
        "\t\t\t\t# the closest k labels\n",
        "\t\t\t\ttemp_list = []\n",
        "\t\t\t\t# count = []\n",
        "\t\t\t\tfor j in range(0, k):\n",
        "\t\t\t\t\ttemp_list.append(self.y[indices[j]])\n",
        "\n",
        "\t\t\t\t# sorts according to the digit that appears tthe most\n",
        "\t\t\t\tfor j in range(0, 10):\n",
        "\n",
        "\t\t\t\t\t#temp stores the no of times j appears\n",
        "\t\t\t\t\ttemp = temp_list.count(j)\n",
        "\t\t\t\t\tif temp == max_count[0]:\n",
        "\t\t\t\t\t\tflag = True\n",
        "\n",
        "\t\t\t\t\telif temp > max_count[0]:\n",
        "\t\t\t\t\t\tflag = False\n",
        "\n",
        "\t\t\t\t\t\t#the no of times j appears\n",
        "\t\t\t\t\t\tmax_count[0] = temp\n",
        "\t\t\t\t\t\t#what the digit is\n",
        "\t\t\t\t\t\tmax_count[1] = j\n",
        "\n",
        "\t\t\t\tif flag:\n",
        "\t\t\t\t\tk -= 1\n",
        "\n",
        "\t\t\t\telif not flag:\n",
        "\t\t\t\t\tans_arr.append(max_count[1])\n",
        "\n",
        "\t\t#the k nearest neighbur for every index\n",
        "\t\treturn ans_arr\n",
        "\n",
        "\n",
        "\t\t# return ???"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0ko5_cLqCId"
      },
      "source": [
        "Optional Challenge:\n",
        "\n",
        "Using for loops can be really slow so in order to improve performance we can leverage the power of vectorization. Try vectorizing the process of finding the distance from a query point to each point in the dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxV02qUIwutD"
      },
      "source": [
        "#### Evaluation\n",
        "\n",
        "Now that you've created a model and \"trained\" it, you can move on to the Evaluation phase.\n",
        "\n",
        "- Implement an `evaluate` function that computes the Confusion Matrix, Accuracy, and Macro-Average F1 score of your classifier.\n",
        "- The function should take as input the predicted labels and the true labels. This will be built in steps: its easier to create a Confusion Matrix, then calculate things like the Precision, Recall and F1 from it.\n",
        "\n",
        "- We will also implement a function that displays our confusion matrix as a heatmap annotated with the data values.\n",
        "- The axes should be properly labelled and the colormap used needs to be shown next to the heatmap.\n",
        "- You can have a look at some examples of heatmaps [here](https://seaborn.pydata.org/generated/seaborn.heatmap.html). (You don't have to use the seaborn libray, but it has some pretty colour palettes to choose from.)\n",
        "\n",
        "We recommend that you do not use hard coding in this function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OGpzp-sw6SZ"
      },
      "outputs": [],
      "source": [
        "def calculate_accuracy(predicted_labels, true_labels):\n",
        "    '''\n",
        "    Returns the accuracy of the predictions against the true labels\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    predicted_labels : array\n",
        "\n",
        "    true_labels : array\n",
        "    \"\"\"\n",
        "    '''\n",
        "    total = len(predicted_labels)\n",
        "    correct = 0\n",
        "\n",
        "    for i in range(0, total):\n",
        "        if predicted_labels[i] == true_labels[i]:\n",
        "            correct += 1\n",
        "\n",
        "    accuracy = (correct / total) * 100\n",
        "    return accuracy\n",
        "    # TODO: Implement a function to calculate accuracy (2 marks)\n",
        "\n",
        "\n",
        "classifier = kNearestNeighbours(5)\n",
        "classifier.fit(input_train, label_train)\n",
        "test = input_test[:100, :]\n",
        "ans = classifier.predict(test, euclidean_distance)\n",
        "test_label = label_test[:300]\n",
        "a = calculate_accuracy(ans,test_label)\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCQtcL-1w6SZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# print(a)\n",
        "def make_confusion_matrix(predicted_labels, true_labels):\n",
        "    '''\n",
        "    Computes the confusion matrix as a 2D array\n",
        "    '''\n",
        "\n",
        "    matrix = np.zeros((10,10))\n",
        "    for i in range(0, len(predicted_labels)):\n",
        "        prediced = int(predicted_labels[i])\n",
        "        true = int(true_labels[i])\n",
        "        matrix[prediced][true] += 1\n",
        "\n",
        "    return matrix\n",
        "\n",
        "\n",
        "    # TODO: Implement a function to compute the confusion matrix (2 marks)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyKsRhQTw6SZ"
      },
      "outputs": [],
      "source": [
        "def make_heat_map(data, title):\n",
        "    \"\"\"\n",
        "    Creates a heatmap from the 2D matrix input\n",
        "    \"\"\"\n",
        "    sns.heatmap(data=data, annot=True)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "    # TODO: Implement a funtion to display a heatmap (2 marks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPvVPmnpw6SZ"
      },
      "outputs": [],
      "source": [
        "def calculate_precision(confusion_matrix,class_label):\n",
        "    '''\n",
        "    Calculates the precision from a provided confusion matrix\n",
        "    '''\n",
        "    p_sum = 0\n",
        "    for i in range(0, 10):\n",
        "        p_sum += confusion_matrix[class_label][i]\n",
        "\n",
        "    ans = confusion_matrix[class_label][class_label] / p_sum\n",
        "\n",
        "    return ans\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GuPdo1jw6Sa"
      },
      "outputs": [],
      "source": [
        "def calculate_recall(confusion_matrix,class_label):\n",
        "    '''\n",
        "    Calculates the recall from a provided confusion matrix\n",
        "    '''\n",
        "    # TODO: Implement a function to compute the recall (2 marks)\n",
        "\n",
        "    r_sum = 0\n",
        "    for i in range(0, 10):\n",
        "        r_sum += confusion_matrix[i][class_label]\n",
        "\n",
        "    ans = confusion_matrix[class_label][class_label] / r_sum\n",
        "\n",
        "    return ans\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olatBHW1w6Sa"
      },
      "outputs": [],
      "source": [
        "\n",
        "def calculate_f1_score(precision, recall):\n",
        "    '''\n",
        "    Calculates the F1 score from a provided precision and recall\n",
        "    '''\n",
        "    numerator = 2 * precision * recall\n",
        "    denominator = precision + recall\n",
        "    # TODO: Implement a function to compute the F1 score (2 marks)\n",
        "\n",
        "    return (numerator / denominator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydkPM5Fgw6Sa"
      },
      "outputs": [],
      "source": [
        "\n",
        "def macro_average_f1(confusion_matrix):\n",
        "    '''\n",
        "    Calculates the macro-average F1 score from a provided confusion matrix, over all classes\n",
        "    '''\n",
        "    # TODO: Implement a function to compute the Macro-average F1 (2 marks)\n",
        "    f1_sum = 0\n",
        "    for i in range(0, 10):\n",
        "        precision = calculate_precision(confusion_matrix, i)\n",
        "        recall = calculate_recall(confusion_matrix, i)\n",
        "        f1 = calculate_f1_score(precision, recall)\n",
        "        f1_sum+= f1\n",
        "\n",
        "\n",
        "    return (f1_sum / 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpnqQnIoN7iu"
      },
      "outputs": [],
      "source": [
        "def evaluation(predicted_labels, true_labels):\n",
        "    \"\"\"\n",
        "    Computes the Confusion Matrix, Accuracy and Macro-average F1 score from the predictions and true labels\n",
        "    \"\"\"\n",
        "    ## Now put it all together using the functions you've already written above.\n",
        "    confusion_matrix = make_confusion_matrix(predicted_labels, true_labels)\n",
        "    accuracy = calculate_accuracy(predicted_labels, true_labels)\n",
        "    macroF1 = macro_average_f1(confusion_matrix)\n",
        "    # TODO: Complete the evaluation function (2 marks)\n",
        "\n",
        "    return confusion_matrix, accuracy, macroF1\n",
        "\n",
        "\n",
        "#########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCMuyMEWeADn"
      },
      "source": [
        "#### `k`-fold Cross Validation\n",
        "\n",
        "<center>\n",
        "    <img src=\"./assets/kfoldcv.png\">\n",
        "</center>\n",
        "\n",
        "Now with the basics done, you can move on to the next step: `k`-fold Cross Validation. This is a more robust way of evaluating your model since it uses all the data for training and testing (effectively giving you `k` chances to verify the generalizability of your model).\n",
        "\n",
        "Now, implement a function that performs `k`-fold cross-validation on the training data for a specified value of `k`.\n",
        "\n",
        "In Cross Validation, you divide the dataset into `k` parts. `k-1` parts will be used for training and `1` part will be used for validation. You will repeat this process `k` times, each time using a different part for validation. You will then average the results of each fold to get the final result. Take a look at the image above for a better understanding.\n",
        "\n",
        "The function should return **predictions** for the **entire training data** (size of list/array should be equal to the size of the dataset). This is the result of appending the predicted labels for each validation-train split into a single list/array. Make sure the order of the predicted labels matches the order of the training dataset, so that they may directly be passed to your `evaluate` function together with the actual labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLDVaOIheMFr"
      },
      "outputs": [],
      "source": [
        "def k_fold_split(k, cv_no, data):\n",
        "    \"\"\"\n",
        "    Returns the training and validation sets for a given value of k\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    k : int\n",
        "        The value of k\n",
        "    cv_no : int\n",
        "        The current fold number\n",
        "    data : array\n",
        "        The dataset\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Implement a function that creates the train and test splits based off the value of k (5 marks)\n",
        "    # Code here\n",
        "    train = []\n",
        "    split_data = np.array_split(data, k)\n",
        "\n",
        "    for i in range(0, k):\n",
        "        if i != cv_no:\n",
        "            train.append(split_data[i])\n",
        "\n",
        "\n",
        "    training_set = np.concatenate(train)\n",
        "    validation_set = split_data[cv_no]\n",
        "\n",
        "    return training_set, validation_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twm1MAloljI0"
      },
      "outputs": [],
      "source": [
        "def k_fold_cross_validation(num_folds, k, dataset, distanceFunction):\n",
        "\t\t\"\"\"\n",
        "\t\tReturns the predictions for all the data points in the dataset using k-fold cross validation\n",
        "\n",
        "\t\tnum_folds: int\n",
        "\t\t\tNumber of folds\n",
        "\t\tk: int\n",
        "\t\t\tNumber of neighbours to consider (hyperparameter)\n",
        "\t\tdataset: array\n",
        "\t\t\tThe dataset to be used (note that this should be the training set which has 11900 samples)\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\t# TODO: Implement function to perform k-fold cross-validation, using the above function (10 marks)\n",
        "\t\t## Code here\n",
        "\n",
        "\t\tprediction_list = []\n",
        "\t\ttrue_list = []\n",
        "\n",
        "\t\tfor i in range(0, num_folds):\n",
        "\t\t\t\ttraining, test = k_fold_split(num_folds, i, dataset)\n",
        "\t\t\t\tclassifier = kNearestNeighbours(k)\n",
        "\t\t\t\tx_train = training[:, 1:]\n",
        "\t\t\t\ty_train = training[:, 0]\n",
        "\t\t\t\tx_test = test[:, 1:]\n",
        "\t\t\t\ty_test = test[:, 0]\n",
        "\t\t\t\tclassifier.fit(x_train, y_train)\n",
        "\t\t\t\tpredict_label = classifier.predict(x_test, distanceFunction)\n",
        "\t\t\t\ttemp = np.array(predict_label)\n",
        "\t\t\t\tprediction_list.append(temp)\n",
        "\t\t\t\ttrue_list.append(y_test)\n",
        "\n",
        "\t\tpred = prediction_list\n",
        "\t\tgold = true_list\n",
        "\n",
        "\t\treturn pred, gold\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWUgqOBHlsyK"
      },
      "source": [
        "Now run your cross-validation function on the training data using `5-fold cross validation` for the values of `k = [1, 2, 3, 4, 5]`.\n",
        "\n",
        "Do this for both the Euclidean distance and the Manhattan distance for each value of `k`.\n",
        "\n",
        "Also run your evaluation function for each value of `k` (for both distance metrics) and print out the classification accuracy and F1 score.\n",
        "\n",
        "(5 marks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7b_CwyoB9gy"
      },
      "outputs": [],
      "source": [
        "# TODO: Perform cross-validation and then run your evaluation function for k=1, printing the accuracy and macro-average F1 score.\n",
        "\n",
        "accuracy_list_euclidean = []\n",
        "macro_list_euclidean = []\n",
        "\n",
        "accuracy_list_manhattan = []\n",
        "macro_list_manhattan = []\n",
        "\n",
        "print(\"k=1 for Euclidean distance\")\n",
        "correct_labels = label_train\n",
        "\n",
        "pred, gold = k_fold_cross_validation(5, 1, data_train, euclidean_distance)\n",
        "\n",
        "# Initialize variables to accumulate results\n",
        "all_confusion_matrices = []\n",
        "all_accuracies = []\n",
        "all_macroF1s = []\n",
        "\n",
        "for i in range(0, len(pred)):\n",
        "\n",
        "    print(f\"Fold {i + 1}:\")\n",
        "    predictions = pred[i]\n",
        "\n",
        "    confusion_matrix, accuracy, macroF1 = evaluation(predictions, gold[i])\n",
        "    all_confusion_matrices.append(confusion_matrix)\n",
        "    all_accuracies.append(accuracy)\n",
        "    all_macroF1s.append(macroF1)\n",
        "    print(f\"Accuracy : {accuracy}\\nMacroF1 : {macroF1}\")\n",
        "\n",
        "# Calculate overall metrics (e.g., average accuracy and macroF1)\n",
        "average_accuracy = sum(all_accuracies) / len(all_accuracies)\n",
        "average_macroF1 = sum(all_macroF1s) / len(all_macroF1s)\n",
        "\n",
        "# Optionally, you can also print the overall metrics\n",
        "print(\"\\nOverall Metrics:\")\n",
        "print(f\"Average Accuracy : {average_accuracy}\")\n",
        "print(f\"Average MacroF1 : {average_macroF1}\")\n",
        "\n",
        "accuracy_list_euclidean.append(average_accuracy)\n",
        "macro_list_euclidean.append(average_macroF1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"k=1 for Manhattan distance\")\n",
        "correct_labels = label_train\n",
        "\n",
        "pred, gold = k_fold_cross_validation(5, 1, data_train, manhattan_distance)\n",
        "\n",
        "# Initialize variables to accumulate results\n",
        "all_confusion_matrices = []\n",
        "all_accuracies = []\n",
        "all_macroF1s = []\n",
        "\n",
        "for i in range(0, len(pred)):\n",
        "\n",
        "    print(f\"Fold {i + 1}:\")\n",
        "    predictions = pred[i]\n",
        "\n",
        "    confusion_matrix, accuracy, macroF1 = evaluation(predictions, gold[i])\n",
        "    all_confusion_matrices.append(confusion_matrix)\n",
        "    all_accuracies.append(accuracy)\n",
        "    all_macroF1s.append(macroF1)\n",
        "    print(f\"Accuracy : {accuracy}\\nMacroF1 : {macroF1}\")\n",
        "\n",
        "# Calculate overall metrics (e.g., average accuracy and macroF1)\n",
        "average_accuracy = sum(all_accuracies) / len(all_accuracies)\n",
        "average_macroF1 = sum(all_macroF1s) / len(all_macroF1s)\n",
        "\n",
        "# Optionally, you can also print the overall metrics\n",
        "print(\"\\nOverall Metrics:\")\n",
        "print(f\"Average Accuracy : {average_accuracy}\")\n",
        "print(f\"Average MacroF1 : {average_macroF1}\")\n",
        "\n",
        "accuracy_list_manhattan.append(average_accuracy)\n",
        "macro_list_manhattan.append(average_macroF1)\n",
        "## Code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prH0556aqCIo"
      },
      "outputs": [],
      "source": [
        "print(\"k=2 for Euclidean distance\")\n",
        "correct_labels = label_train\n",
        "\n",
        "pred, gold = k_fold_cross_validation(5, 2, data_train, euclidean_distance)\n",
        "\n",
        "# Initialize variables to accumulate results\n",
        "all_confusion_matrices = []\n",
        "all_accuracies = []\n",
        "all_macroF1s = []\n",
        "\n",
        "for i in range(0, len(pred)):\n",
        "\n",
        "    print(f\"Fold {i + 1}:\")\n",
        "    predictions = pred[i]\n",
        "\n",
        "    confusion_matrix, accuracy, macroF1 = evaluation(predictions, gold[i])\n",
        "    all_confusion_matrices.append(confusion_matrix)\n",
        "    all_accuracies.append(accuracy)\n",
        "    all_macroF1s.append(macroF1)\n",
        "    print(f\"Accuracy : {accuracy}\\nMacroF1 : {macroF1}\")\n",
        "\n",
        "# Calculate overall metrics (e.g., average accuracy and macroF1)\n",
        "average_accuracy = sum(all_accuracies) / len(all_accuracies)\n",
        "average_macroF1 = sum(all_macroF1s) / len(all_macroF1s)\n",
        "\n",
        "# Optionally, you can also print the overall metrics\n",
        "print(\"\\nOverall Metrics:\")\n",
        "print(f\"Average Accuracy : {average_accuracy}\")\n",
        "print(f\"Average MacroF1 : {average_macroF1}\")\n",
        "\n",
        "accuracy_list_euclidean.append(average_accuracy)\n",
        "macro_list_euclidean.append(average_macroF1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"k=2 for Manhattan distance\")\n",
        "correct_labels = label_train\n",
        "\n",
        "pred, gold = k_fold_cross_validation(5, 2, data_train, manhattan_distance)\n",
        "\n",
        "# Initialize variables to accumulate results\n",
        "all_confusion_matrices = []\n",
        "all_accuracies = []\n",
        "all_macroF1s = []\n",
        "\n",
        "for i in range(0, len(pred)):\n",
        "\n",
        "    print(f\"Fold {i + 1}:\")\n",
        "    predictions = pred[i]\n",
        "\n",
        "    confusion_matrix, accuracy, macroF1 = evaluation(predictions, gold[i])\n",
        "    all_confusion_matrices.append(confusion_matrix)\n",
        "    all_accuracies.append(accuracy)\n",
        "    all_macroF1s.append(macroF1)\n",
        "    print(f\"Accuracy : {accuracy}\\nMacroF1 : {macroF1}\")\n",
        "\n",
        "# Calculate overall metrics (e.g., average accuracy and macroF1)\n",
        "average_accuracy = sum(all_accuracies) / len(all_accuracies)\n",
        "average_macroF1 = sum(all_macroF1s) / len(all_macroF1s)\n",
        "\n",
        "# Optionally, you can also print the overall metrics\n",
        "print(\"\\nOverall Metrics:\")\n",
        "print(f\"Average Accuracy : {average_accuracy}\")\n",
        "print(f\"Average MacroF1 : {average_macroF1}\")\n",
        "\n",
        "accuracy_list_manhattan.append(average_accuracy)\n",
        "macro_list_manhattan.append(average_macroF1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWe_IEc4qCIp"
      },
      "outputs": [],
      "source": [
        "print(\"k=3 for Euclidean distance\")\n",
        "correct_labels = label_train\n",
        "\n",
        "pred, gold = k_fold_cross_validation(5, 3, data_train, euclidean_distance)\n",
        "\n",
        "# Initialize variables to accumulate results\n",
        "all_confusion_matrices = []\n",
        "all_accuracies = []\n",
        "all_macroF1s = []\n",
        "\n",
        "for i in range(0, len(pred)):\n",
        "\n",
        "    print(f\"Fold {i + 1}:\")\n",
        "    predictions = pred[i]\n",
        "\n",
        "    confusion_matrix, accuracy, macroF1 = evaluation(predictions, gold[i])\n",
        "    all_confusion_matrices.append(confusion_matrix)\n",
        "    all_accuracies.append(accuracy)\n",
        "    all_macroF1s.append(macroF1)\n",
        "    print(f\"Accuracy : {accuracy}\\nMacroF1 : {macroF1}\")\n",
        "\n",
        "# Calculate overall metrics (e.g., average accuracy and macroF1)\n",
        "average_accuracy = sum(all_accuracies) / len(all_accuracies)\n",
        "average_macroF1 = sum(all_macroF1s) / len(all_macroF1s)\n",
        "\n",
        "# Optionally, you can also print the overall metrics\n",
        "print(\"\\nOverall Metrics:\")\n",
        "print(f\"Average Accuracy : {average_accuracy}\")\n",
        "print(f\"Average MacroF1 : {average_macroF1}\")\n",
        "\n",
        "accuracy_list_euclidean.append(average_accuracy)\n",
        "macro_list_euclidean.append(average_macroF1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"k=3 for Manhattan distance\")\n",
        "correct_labels = label_train\n",
        "\n",
        "pred, gold = k_fold_cross_validation(5, 3, data_train, manhattan_distance)\n",
        "\n",
        "# Initialize variables to accumulate results\n",
        "all_confusion_matrices = []\n",
        "all_accuracies = []\n",
        "all_macroF1s = []\n",
        "\n",
        "for i in range(0, len(pred)):\n",
        "\n",
        "    print(f\"Fold {i + 1}:\")\n",
        "    predictions = pred[i]\n",
        "\n",
        "    confusion_matrix, accuracy, macroF1 = evaluation(predictions, gold[i])\n",
        "    all_confusion_matrices.append(confusion_matrix)\n",
        "    all_accuracies.append(accuracy)\n",
        "    all_macroF1s.append(macroF1)\n",
        "    print(f\"Accuracy : {accuracy}\\nMacroF1 : {macroF1}\")\n",
        "\n",
        "# Calculate overall metrics (e.g., average accuracy and macroF1)\n",
        "average_accuracy = sum(all_accuracies) / len(all_accuracies)\n",
        "average_macroF1 = sum(all_macroF1s) / len(all_macroF1s)\n",
        "\n",
        "# Optionally, you can also print the overall metrics\n",
        "print(\"\\nOverall Metrics:\")\n",
        "print(f\"Average Accuracy : {average_accuracy}\")\n",
        "print(f\"Average MacroF1 : {average_macroF1}\")\n",
        "\n",
        "accuracy_list_manhattan.append(average_accuracy)\n",
        "macro_list_manhattan.append(average_macroF1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOFvsSWQqCIq"
      },
      "outputs": [],
      "source": [
        "print(\"k=4 for Euclidean distance\")\n",
        "correct_labels = label_train\n",
        "\n",
        "pred, gold = k_fold_cross_validation(5, 4, data_train, euclidean_distance)\n",
        "\n",
        "# Initialize variables to accumulate results\n",
        "all_confusion_matrices = []\n",
        "all_accuracies = []\n",
        "all_macroF1s = []\n",
        "\n",
        "for i in range(0, len(pred)):\n",
        "\n",
        "    print(f\"Fold {i + 1}:\")\n",
        "    predictions = pred[i]\n",
        "\n",
        "    confusion_matrix, accuracy, macroF1 = evaluation(predictions, gold[i])\n",
        "    all_confusion_matrices.append(confusion_matrix)\n",
        "    all_accuracies.append(accuracy)\n",
        "    all_macroF1s.append(macroF1)\n",
        "    print(f\"Accuracy : {accuracy}\\nMacroF1 : {macroF1}\")\n",
        "\n",
        "# Calculate overall metrics (e.g., average accuracy and macroF1)\n",
        "average_accuracy = sum(all_accuracies) / len(all_accuracies)\n",
        "average_macroF1 = sum(all_macroF1s) / len(all_macroF1s)\n",
        "\n",
        "# Optionally, you can also print the overall metrics\n",
        "print(\"\\nOverall Metrics:\")\n",
        "print(f\"Average Accuracy : {average_accuracy}\")\n",
        "print(f\"Average MacroF1 : {average_macroF1}\")\n",
        "\n",
        "accuracy_list_euclidean.append(average_accuracy)\n",
        "macro_list_euclidean.append(average_macroF1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"k=4 for Manhattan distance\")\n",
        "correct_labels = label_train\n",
        "\n",
        "pred, gold = k_fold_cross_validation(5, 4, data_train, manhattan_distance)\n",
        "\n",
        "# Initialize variables to accumulate results\n",
        "all_confusion_matrices = []\n",
        "all_accuracies = []\n",
        "all_macroF1s = []\n",
        "\n",
        "for i in range(0, len(pred)):\n",
        "\n",
        "    print(f\"Fold {i + 1}:\")\n",
        "    predictions = pred[i]\n",
        "\n",
        "    confusion_matrix, accuracy, macroF1 = evaluation(predictions, gold[i])\n",
        "    all_confusion_matrices.append(confusion_matrix)\n",
        "    all_accuracies.append(accuracy)\n",
        "    all_macroF1s.append(macroF1)\n",
        "    print(f\"Accuracy : {accuracy}\\nMacroF1 : {macroF1}\")\n",
        "\n",
        "# Calculate overall metrics (e.g., average accuracy and macroF1)\n",
        "average_accuracy = sum(all_accuracies) / len(all_accuracies)\n",
        "average_macroF1 = sum(all_macroF1s) / len(all_macroF1s)\n",
        "\n",
        "# Optionally, you can also print the overall metrics\n",
        "print(\"\\nOverall Metrics:\")\n",
        "print(f\"Average Accuracy : {average_accuracy}\")\n",
        "print(f\"Average MacroF1 : {average_macroF1}\")\n",
        "\n",
        "accuracy_list_manhattan.append(average_accuracy)\n",
        "macro_list_manhattan.append(average_macroF1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrJ9KJimqCIr"
      },
      "outputs": [],
      "source": [
        "print(\"k=5 for Euclidean distance\")\n",
        "correct_labels = label_train\n",
        "\n",
        "pred, gold = k_fold_cross_validation(5, 5, data_train, euclidean_distance)\n",
        "\n",
        "# Initialize variables to accumulate results\n",
        "all_confusion_matrices = []\n",
        "all_accuracies = []\n",
        "all_macroF1s = []\n",
        "\n",
        "for i in range(0, len(pred)):\n",
        "\n",
        "    print(f\"Fold {i + 1}:\")\n",
        "    predictions = pred[i]\n",
        "\n",
        "    confusion_matrix, accuracy, macroF1 = evaluation(predictions, gold[i])\n",
        "    all_confusion_matrices.append(confusion_matrix)\n",
        "    all_accuracies.append(accuracy)\n",
        "    all_macroF1s.append(macroF1)\n",
        "    print(f\"Accuracy : {accuracy}\\nMacroF1 : {macroF1}\")\n",
        "\n",
        "# Calculate overall metrics (e.g., average accuracy and macroF1)\n",
        "average_accuracy = sum(all_accuracies) / len(all_accuracies)\n",
        "average_macroF1 = sum(all_macroF1s) / len(all_macroF1s)\n",
        "\n",
        "# Optionally, you can also print the overall metrics\n",
        "print(\"\\nOverall Metrics:\")\n",
        "print(f\"Average Accuracy : {average_accuracy}\")\n",
        "print(f\"Average MacroF1 : {average_macroF1}\")\n",
        "\n",
        "accuracy_list_euclidean.append(average_accuracy)\n",
        "macro_list_euclidean.append(average_macroF1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"k=5 for Manhattan distance\")\n",
        "correct_labels = label_train\n",
        "\n",
        "pred, gold = k_fold_cross_validation(5, 5, data_train, manhattan_distance)\n",
        "\n",
        "# Initialize variables to accumulate results\n",
        "all_confusion_matrices = []\n",
        "all_accuracies = []\n",
        "all_macroF1s = []\n",
        "\n",
        "for i in range(0, len(pred)):\n",
        "\n",
        "    print(f\"Fold {i + 1}:\")\n",
        "    predictions = pred[i]\n",
        "\n",
        "    confusion_matrix, accuracy, macroF1 = evaluation(predictions, gold[i])\n",
        "    all_confusion_matrices.append(confusion_matrix)\n",
        "    all_accuracies.append(accuracy)\n",
        "    all_macroF1s.append(macroF1)\n",
        "    print(f\"Accuracy : {accuracy}\\nMacroF1 : {macroF1}\")\n",
        "\n",
        "# Calculate overall metrics (e.g., average accuracy and macroF1)\n",
        "average_accuracy = sum(all_accuracies) / len(all_accuracies)\n",
        "average_macroF1 = sum(all_macroF1s) / len(all_macroF1s)\n",
        "\n",
        "# Optionally, you can also print the overall metrics\n",
        "print(\"\\nOverall Metrics:\")\n",
        "print(f\"Average Accuracy : {average_accuracy}\")\n",
        "print(f\"Average MacroF1 : {average_macroF1}\")\n",
        "\n",
        "accuracy_list_manhattan.append(average_accuracy)\n",
        "macro_list_manhattan.append(average_macroF1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZH6EG8d0R4s"
      },
      "source": [
        "Next, present the results as a graph with `k` values on the x-axis and classification accuracy on the y-axis.\n",
        "\n",
        "Use a single plot to compare the two versions of the classifier (one using Euclidean and the other using Manhattan distance metric). Make another graph but with the F1-score on the y-axis this time. The graphs should be properly labeled.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcUclERi0fmg"
      },
      "outputs": [],
      "source": [
        "## (3 marks)\n",
        "# TODO: Plot a graph with k values on the x-axis and classification accuracy on the y-axis\n",
        "\n",
        "k_values = np.arange(1, 6)  # k values from 1 to 5\n",
        "\n",
        "# Create a single plot to compare the two versions\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_values, accuracy_list_euclidean, marker='o', label='Euclidean Distance')\n",
        "plt.plot(k_values, accuracy_list_manhattan, marker='o', label='Manhattan Distance')\n",
        "\n",
        "plt.title('Classification Accuracy vs. k Value')\n",
        "plt.xlabel('k Value')\n",
        "plt.ylabel('Classification Accuracy')\n",
        "plt.xticks(k_values)\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lVZA1m1w6Sh"
      },
      "outputs": [],
      "source": [
        "## (3 marks)\n",
        "# TODO: Plot a graph with k values on the x-axis and F1-score on the y-axis\n",
        "\n",
        "k_values = np.arange(1, 6)  # k values from 1 to 5\n",
        "\n",
        "# Create a single plot to compare the two versions\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_values, macro_list_euclidean, marker='o', label='Euclidean Distance')\n",
        "plt.plot(k_values, macro_list_manhattan, marker='o', label='Manhattan Distance')\n",
        "\n",
        "plt.title('Classification F1 vs. k Value')\n",
        "plt.xlabel('k Value')\n",
        "plt.ylabel('F1 Accuracy')\n",
        "plt.xticks(k_values)\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MjHsLCx1Dic"
      },
      "source": [
        "Comment on the best value of k you have found for both distance metrics using\n",
        "cross-validation. What impact does this value have on the decision boundries generated by the model and the stability of decisions?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JDqvNjX1KUt"
      },
      "outputs": [],
      "source": [
        "## (2 marks)\n",
        "# TODO: Write your answer below as a comment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eulkPB4y1dgM"
      },
      "source": [
        "Finally, use the best value of `k` for both distance metrics and run it on the test dataset.\n",
        "\n",
        "Find the confusion matrix, classification accuracy and F1 score and print them.\n",
        "\n",
        "The confusion matrix must be displayed as a heatmap annotated with the data values. The axes should be properly labelled and the colormap used needs to be shown next to the heatmap.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iU40KNeb80Xa"
      },
      "outputs": [],
      "source": [
        "## (3 marks)\n",
        "# TODO: Use the best value of k on test dataset (for both distance metrics).\n",
        "\n",
        "\n",
        "# Code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZwA3ffh9ZfK"
      },
      "source": [
        "## **Part 2: Implement using Scikit-Learn (25 marks)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SB6Uqhlt9n7P"
      },
      "source": [
        "In this part, you have to use [scikit-learn's k-NN implementation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) to train and test your classifier on the dataset used in Part 1. Repeat the tasks you have done in Part 1 but this time using scikit-learn.\n",
        "\n",
        "- Perform 5-fold cross-validation and run the k-NN classifier for values of `k = [1, 2, 3, 4, 5]` using both Euclidean and Manhattan distance.\n",
        "\n",
        "- Use scikit-learn's [accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) function to calculate the accuracy, the [classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) to calculate macro-average F1 score,\n",
        "  and the [confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) function to calculate confusion matrix from the predicted labels.\n",
        "\n",
        "- Present the results as a graph with k values on the x-axis and performance measures on the y-axis just like you did in Part 1. Use a single plot to compare the two versions of the classifier (one using Euclidean and the other using Manhattan distance metric).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a39IXBGnqCI0"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0_GlUmZw6Si"
      },
      "outputs": [],
      "source": [
        "# (10 marks)\n",
        "# TODO:  Perform 5-fold cross-validation.\n",
        "# Code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jydXs3tyw6Si"
      },
      "outputs": [],
      "source": [
        "# (5 marks)\n",
        "# TODO: Plot a graph with k values on the x-axis and classifcation accuracy on the y-axis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VA8Qn8XhqCI3"
      },
      "outputs": [],
      "source": [
        "## (5 marks)\n",
        "# TODO: Plot a graph with k values on the x-axis and F1-score on the y-axis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSk2JJzjqCI5"
      },
      "source": [
        "Finally, print the best values of k for both distance metrics. Then use these values of k on the test dataset and print the evaluation scores and confusion matrix (as a heatmap) for each of the distance metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZMnr9KgqCI7"
      },
      "outputs": [],
      "source": [
        "## (5 marks)\n",
        "# TODO: Use the best value of k on test dataset (for both distance metrics).\n",
        "\n",
        "# Code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baKb0CRZ7uqU"
      },
      "source": [
        "# Testing the classifier with your own handwriting!\n",
        "\n",
        "Gradio is an open-source Python library that is used to build machine learning and data science demos and web applications.\n",
        "\n",
        "We can use the sketchpad interface to write our digits and pass that to our classifier. Try it out below!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuDkj5rr7tQ-"
      },
      "outputs": [],
      "source": [
        "!pip install gradio\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52hF4s26UG3p"
      },
      "outputs": [],
      "source": [
        "#### Initialize your classifier here ####\n",
        "\n",
        "#### Try this with out with your own classifiers\n",
        "\n",
        "my_classifier = kNearestNeighbours(???)\n",
        "my_classifier.fit(x_train,y_train)\n",
        "\n",
        "my_sk_classifer = KNeighborsClassifier(n_neighbors=???,metric=???)\n",
        "my_sk_classifer.fit(x_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSbEjbZ57tRN"
      },
      "outputs": [],
      "source": [
        "def sketch_recognition(img):\n",
        "    image_arr = img.flatten()\n",
        "\n",
        "    # label = my_classifier.predict([image_arr],EuclideanDistance)\n",
        "\n",
        "    label = my_sk_classifer.predict([image_arr])\n",
        "\n",
        "    print(\"Your label: \", label[0])\n",
        "    return str(label[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wn2EE8ONUWG7"
      },
      "outputs": [],
      "source": [
        "iface = gr.Interface(fn=sketch_recognition, inputs=\"sketchpad\", outputs=\"text\")\n",
        "iface.launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZ5EqZS0Sd7g"
      },
      "outputs": [],
      "source": [
        "### Draw and save your handwritten digits as an image (if you want)\n",
        "\n",
        "gr.Interface(lambda x: x, \"sketchpad\", \"image\").launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0-gnRzZqCI_"
      },
      "outputs": [],
      "source": [
        "## (5 marks)\n",
        "## TODO: Use images of your own handwritten digits and make a prediction using your own classifier.\n",
        "## You can use the sketchpad above if you'd like or even use pencil and paper to write out some numbers and then take a picture.\n",
        "\n",
        "\n",
        "## Display five of these images and print out the corresponding prediction\n",
        "## You can use either your own classifier or sklearn.\n",
        "\n",
        "## Have fun!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ziv0p49qCJA"
      },
      "source": [
        "## Fin.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}